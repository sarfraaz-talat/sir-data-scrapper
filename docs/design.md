# ğŸ§¾ **Design Document â€” SIR Data Scraper**

## 1. Overview

### 1.1 Project Summary

**SIR Data Scraper** is a Python-based ETL pipeline designed to **automate extraction, parsing, and structuring** of voter roll data published on the *Election Commission of Indiaâ€™s Search-in-Roll (SIR)* portal.

It systematically downloads voter list PDFs (via ZIP archives) for every state and assembly constituency, extracts structured data (voter name, EPIC number, address, etc.), optionally translates fields into English, and stores the results into a relational database with strong indexing for fast retrieval and filtering.

---

## 2. Objectives

| Goal                    | Description                                                                             |
| ----------------------- | --------------------------------------------------------------------------------------- |
| **Automated ingestion** | Fully automate download of all state/assembly voter rolls from the SIR portal           |
| **Structured storage**  | Convert unstructured PDFs to tabular SQL data                                           |
| **Language-neutral**    | Support all Indian languages (OG = Original language) with optional English translation |
| **Scalable**            | Process multiple states and assemblies in batches, asynchronously                       |
| **Resilient**           | Recover from interruptions via checkpointing and resumable logs                         |
| **Auditable**           | Maintain progressive logs and optional full log dumps (`--savelogs` flag)               |

---

## 3. High-Level Architecture

**3-Stage Pipeline Architecture:**

```
+---------------------------------------------------------------+
|                         SIR Portal                            |
| (https://voters.eci.gov.in/searchInSIR/S2UA4DPDF-JK4QWODSE)   |
+---------------------------------------------------------------+
                  â”‚
                  â–¼
+--------------------------------+
| Crawler (Playwright)           |
| â†’ Extract ZIP download URLs    |
| â†’ Group by State/Assembly      |
+--------------------------------+
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STAGE 1: DOWNLOAD                        â”‚
â”‚  Downloader (aiohttp) - Parallel downloads                  â”‚
â”‚  â†’ Skip if file exists                                       â”‚
â”‚  â†’ Retry on failure                                          â”‚
â”‚  â†’ Save checkpoint after each constituency                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STAGE 2: PARSE                           â”‚
â”‚  Extractor â†’ Unzip ZIPs to PDFs                            â”‚
â”‚  Parser (pdfplumber / PyMuPDF / OCR) - Parallel parsing    â”‚
â”‚  â†’ Extract voter data with regex                            â”‚
â”‚  â†’ Extract EPIC prefix, address components                   â”‚
â”‚  â†’ Keep records without EPIC                                â”‚
â”‚  â†’ Save checkpoint after parsing                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STAGE 3: STORE                           â”‚
â”‚  Translator (optional) â†’ Translate OG â†’ English            â”‚
â”‚  DB Loader (SQLAlchemy) â†’ Store in SQLite                  â”‚
â”‚  â†’ Unique ID (UUID) as primary key                          â”‚
â”‚  â†’ EPIC nullable (allows duplicates)                        â”‚
â”‚  â†’ Save checkpoint after storage                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
+--------------------------------+
| Checkpoint Manager             |
| â†’ latest.json (current state)  |
| â†’ Timestamped history files    |
| â†’ Resume support               |
+--------------------------------+
```

---

## 4. Core Components & Responsibilities

### 4.1 `crawler.py`

* Uses **Playwright** to navigate `SIR` dropdowns.
* Iterates through each **state**, loads assemblies, and extracts all **ZIP/PDF download links**.
* Yields structured metadata:

  ```json
  {
    "state": "Gujarat",
    "assembly": "Maninagar",
    "url": "https://.../Part42.zip"
  }
  ```
* Saves checkpoint after every state.

### 4.2 `downloader.py`

* Async batch downloader using `aiohttp` with parallel downloads (default 5 concurrent).
* **Skips existing files automatically** (checks file size to ensure completeness).
* Writes ZIPs to:

  ```
  ./data/voterlists/<State>/<Assembly>/<file>.zip
  ```
* Retries up to 3 times on network failures.
* Emits progressive logs: `[Downloading]`, `[Success]`, `[Retry]`, `[Failed]`.

### 4.3 `extractor.py`

* Unzips all files under each assembly.
* Validates extracted PDFs (by extension and basic file integrity).
* Removes successfully processed ZIPs.
* Updates manifest (JSON):

  ```json
  {"Gujarat": {"Maninagar": ["Part1.pdf", "Part2.pdf"]}}
  ```

### 4.4 `parser.py`

* Handles text extraction with fallback chain:

  1. **PyMuPDF** (fitz) - Better for CID-encoded fonts
  2. **pdfplumber** - Standard text extraction
  3. **pytesseract** (OCR) - Fallback for garbled text/images
* Regex-based field extraction with multilingual support:

  ```
  EPIC: ([A-Z]{2}/\d{2}/)?([A-Z]{3}\d{7}|\d{3}/\d{6})  # With prefix
  Name: (.*)
  Relation Type: (Father|Husband|Mother)
  Relation Name: (.*)
  Age: (\d+)
  Gender: (Male|Female|Other|àªªà«àª°à«àª·|àª¸à«àª¤à«àª°à«€)
  Address: (House, Area, Village, Taluka, District)
  ```
* **Extracts EPIC prefix** from PDF headers (e.g., "GJ/01").
* **Extracts address components** (house, area, village, taluka, district).
* **Keeps records without EPIC** (EPIC is nullable).
* **Parallel parsing** within each constituency (configurable workers).
* Creates structured Python dicts for each voter.

### 4.5 `translator.py`

* Uses `deep-translator` (GoogleTranslator) for translation.
* Batch translates fields: `name_og`, `relation_og`, `address_og` â†’ `_en`.
* Optional toggle via `--translate`.
* Gracefully handles missing translation library.

### 4.6 `db_loader.py`

* Uses SQLAlchemy ORM for schema management.
* **Unique ID (UUID)** as primary key (not EPIC).
* **EPIC is nullable** (allows duplicates and missing EPICs).
* Inserts data in batches (no UPSERT - each record gets unique ID).
* Creates indexes for efficient filtering:

  ```sql
  CREATE INDEX idx_epic_no ON voters(epic_no);
  CREATE INDEX idx_state ON voters(state);
  CREATE INDEX idx_assembly ON voters(assembly);
  CREATE INDEX idx_state_assembly ON voters(state, assembly);
  ```

### 4.7 `logger.py`

* Uses Pythonâ€™s `logging` + `rich` for colorful console output.
* Progressive log stream always enabled.
* If `--savelogs` is passed:

  * Dumps full structured logs into `/logs/run_<timestamp>.log`
  * Includes timings, record counts, warnings, failures, etc.

---

## 5. Data Model

### SQL Table: `voters`

| Column         | Type      | Description                                    |
| -------------- | --------- | ---------------------------------------------- |
| `id`           | TEXT (PK) | Unique UUID (generated at runtime)             |
| `epic_no`      | TEXT      | EPIC voter ID (nullable, allows duplicates)    |
| `name_og`      | TEXT      | Name in original language                      |
| `name_en`      | TEXT      | English translation                            |
| `relation_type`| TEXT      | Relation type (Father/Husband/Mother)          |
| `relation_og`  | TEXT      | Relation name (OG)                             |
| `relation_en`  | TEXT      | Relation (EN)                                  |
| `age`          | INTEGER   |                                                |
| `gender`       | TEXT      |                                                |
| `address_og`   | TEXT      | Combined address (OG)                          |
| `address_en`   | TEXT      | Combined address (EN)                          |
| `state`        | TEXT      |                                                |
| `assembly`     | TEXT      |                                                |
| `source_file`  | TEXT      | Source PDF name                                |
| `last_updated` | DATETIME  | Timestamp                                      |

---

## 6. Command-Line Interface (CLI)

### **Usage**

```bash
python main.py [options]
```

### **Arguments**

| Flag                   | Description                                    |
| ---------------------- | ---------------------------------------------- |
| `--state <name>`       | Limit to a specific state                      |
| `--max-assemblies <n>` | Limit assemblies (for testing)                 |
| `--translate`          | Enable OG â†’ English translation                |
| `--savelogs`           | Save extended logs to file                     |
| `--resume`             | Continue from last checkpoint (checks latest.json) |
| `--db <path>`          | Custom DB path (default `data/voters.db`)      |
| `--parse-workers <n>`  | Number of parallel workers for parsing (default: 4) |
| `--show-browser`       | Show browser window (debug mode)               |

### **Examples**

```bash
# Download all Gujarat assemblies
python main.py --state Gujarat --savelogs

# Full India run (resume if interrupted)
python main.py --resume --translate --savelogs
```

---

## 7. Logging Design

### **Progressive Output (always on)**

```
[STATE: Gujarat] [ASSEMBLY: Maninagar]
â†’ Found 42 ZIPs
â†’ Downloading Part12.zip... âœ… 12.4 MB
â†’ Extracted 15 PDFs
â†’ Parsed 3,842 records
â†’ Inserted: 3,828 new | 14 updated
```

### **Saved Logs (`--savelogs`)**

JSON file example:

```json
{
  "run_id": "2025-11-11T19:45:22",
  "state": "Gujarat",
  "assembly": "Maninagar",
  "files_downloaded": 42,
  "pdfs_extracted": 351,
  "records_inserted": 38428,
  "errors": 2,
  "duration_sec": 1765.4
}
```

---

## 8. Error Handling & Resilience

| Scenario             | Behavior                              |
| -------------------- | ------------------------------------- |
| Network timeout      | Retry (max 3) then mark failed        |
| ZIP corrupted        | Skip, log warning                     |
| OCR failure          | Mark partial parse, continue          |
| DB lock / busy       | Retry with backoff                    |
| Script crash         | Resume via checkpoint JSON            |
| Manual stop (Ctrl+C) | Graceful shutdown and checkpoint save |

---

## 9. Storage Layout

```
sir-data-scraper/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ voterlists/
â”‚   â”‚   â”œâ”€â”€ Gujarat/
â”‚   â”‚   â”‚   â”œâ”€â”€ Maninagar/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Part1.pdf
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Part2.pdf
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ Maharashtra/...
â”‚   â””â”€â”€ voters.db
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ run_2025-11-11T19-45-22.log
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ crawler.py
â”‚   â”œâ”€â”€ downloader.py
â”‚   â”œâ”€â”€ extractor.py
â”‚   â”œâ”€â”€ parser.py
â”‚   â”œâ”€â”€ translator.py
â”‚   â”œâ”€â”€ db_loader.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ pipeline.py          # 3-stage pipeline
â”‚   â””â”€â”€ checkpoint.py        # Checkpoint management
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ checkpoints/
â”‚       â”œâ”€â”€ latest.json       # Current state
â”‚       â””â”€â”€ history/          # Timestamped checkpoints
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py                   # Main entry point
â””â”€â”€ process_existing_pdfs.py  # Process existing PDFs
```

---

## 10. Performance & Scaling

| Optimization                    | Description                            |
| ------------------------------- | -------------------------------------- |
| Async IO                        | Parallel downloads (5 concurrent)      |
| Parallel parsing                | ThreadPoolExecutor (configurable workers) |
| Batch DB inserts                | Reduces I/O overhead                   |
| Regex precompilation            | Faster text parsing                    |
| Incremental resume              | Checkpoint system (latest.json)        |
| Skip existing files             | Automatic skip for already-downloaded  |
| Constituency-wise processing    | One constituency at a time (data integrity) |

---

## 11. Future Enhancements

1. Add language auto-detection for OG text.
2. Move translation to local model (IndicTrans2).
3. Expose pipeline as a REST or gRPC ingestion API.
4. Integrate directly with the **sir-lookup** live DB.
5. Add dashboard (e.g., Streamlit) for monitoring ingestion progress.

---

## 12. Security & Compliance

* Project is for **publicly available voter roll data** (no private API scraping).
* Only processes government-published PDFs.
* Each run should include a disclaimer in the README:

  > â€œThis tool automates processing of publicly available electoral roll PDFs from the ECIâ€™s official SIR portal. No unauthorized data or credentials are accessed.â€

---

## 13. Summary

| Component    | Technology                    | Description                     |
| ------------ | ----------------------------- | ------------------------------- |
| Web scraping | Playwright                    | Extract ZIP URLs from dropdowns |
| Download     | aiohttp                       | Parallel async downloading      |
| Parsing      | PyMuPDF / pdfplumber / OCR    | Extract text with fallback chain|
| Translation  | deep-translator               | Optional multilingual support   |
| Database     | SQLAlchemy + SQLite           | Fast local store (UUID PK)      |
| Checkpoint   | JSON (latest.json)            | Resume support                  |
| Pipeline     | 3-stage architecture          | Download â†’ Parse â†’ Store        |
| Logging      | rich + logging                | Real-time + savelogs            |
| CLI          | argparse                      | Clean user control              |
